import dlt
from pyspark.sql.functions import current_timestamp, input_file_name

@dlt.table(
    name="region",
    comment="Ingested region data from source to bronze",
    table_properties={"quality": "bronze"}
)
def load_region():
    df = (
        spark.read.format("parquet")
        .load("abfss://source@endtoendadls.dfs.core.windows.net/regions")
        .withColumn("file_name", input_file_name())
        .withColumn("ingestion_ts", current_timestamp())
    )
    return df